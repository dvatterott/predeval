{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, I demonstrate how to use predeval with a model producing continuous outputs. This example uses the [diabetes dataset from sklearn](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_diabetes.html#sklearn.datasets.load_diabetes). I start by loading this dataset and using the first 300 samples to train a linear regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression(copy_X=True, fit_intercept=True, n_jobs=1, normalize=False)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn import datasets\n",
    "import numpy as np\n",
    "from numpy.random import seed\n",
    "\n",
    "# load data\n",
    "diabetes = datasets.load_diabetes()\n",
    "all_x = diabetes.data\n",
    "all_y = diabetes.target\n",
    "\n",
    "# shuffle data\n",
    "indices = np.arange(all_x.shape[0])\n",
    "seed(1234)\n",
    "np.random.shuffle(indices)\n",
    "\n",
    "# create training set\n",
    "X = all_x[indices[:300], :]\n",
    "Y = all_y[indices[:300]]\n",
    "\n",
    "# train model\n",
    "linreg = LinearRegression()\n",
    "linreg.fit(X, Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a trained model, we can use the model's output to create a ContinuousEvaluator object. We start by importing the ContinuousEvaluator class and instantiating an object, <code>ce</code>, with the model's output. Predeval uses this data to form expectations about how future data will look."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from predeval import ContinuousEvaluator\n",
    "\n",
    "# give model output from training data to ContinuousEvaluator object\n",
    "model_output = linreg.predict(X)\n",
    "ce = ContinuousEvaluator(model_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a real setting, we would give the model completely new data and pass its new predictions into `ce`. Instead, we will pass the remaining data to the model. We will then ask predeval to compare these new outputs to the old outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Passed min check; min observed=52.1812\n",
      "Passed max check; max observed=288.6014\n",
      "Passed mean check; mean observed=159.6063 (Expected 152.7400 +- 104.0139)\n",
      "Passed std check; std observed=53.4388 (Expected 52.0070 +- 26.0035)\n",
      "Passed ks check; test statistic=0.0937, p=0.3497\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('min', True), ('max', True), ('mean', True), ('std', True), ('ks', True)]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# give validation data to ContinuousEvaluator object\n",
    "new_model_output = linreg.predict(all_x[300:, :])\n",
    "ce.check_data(new_model_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not surprisingly (given that we don't expect the first 300 samples to differ from the remaining samples), predeval did not find a difference between the model outputs. \n",
    "\n",
    "Let's say that one of your features goes bad. Specifically, let's say the first feature becomes entirely populated by 100s. We didn't catch this mistake, and are feeding bad data to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed min check; min observed=-2796.7910\n",
      "Passed max check; max observed=-2557.8884\n",
      "Failed mean check; mean observed=-2687.6767 (Expected 152.7400 +- 104.0139)\n",
      "Passed std check; std observed=53.8577 (Expected 52.0070 +- 26.0035)\n",
      "Failed ks check; test statistic=1.0000, p=0.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('min', False), ('max', True), ('mean', False), ('std', True), ('ks', False)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# screw up validation data and give screwed up data to ContinuousEvaluator\n",
    "all_x[300:, 0] = 100\n",
    "new_model_output_bad = linreg.predict(all_x[300:, :])\n",
    "ce.check_data(new_model_output_bad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, predeval detects a change in the model's output. This is a overly-dramatic example, but hopefully predeval can help you find more subtle changes."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, I demonstrate how to use predeval with a model producing categorical outputs. This example uses the [iris dataset from sklearn](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_iris.html). I start by loading this dataset and using the first 100 samples to train a logistic regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=100000.0, class_weight=None, dual=False,\n",
       "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
       "          multi_class='multinomial', n_jobs=1, penalty='l2',\n",
       "          random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "          warm_start=False)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import datasets\n",
    "import numpy as np\n",
    "from numpy.random import seed\n",
    "\n",
    "# load data\n",
    "iris = datasets.load_iris()\n",
    "all_x = iris.data[:, :2]\n",
    "all_y = iris.target\n",
    "\n",
    "# shuffle data\n",
    "indices = np.arange(all_x.shape[0])\n",
    "seed(1234)\n",
    "np.random.shuffle(indices)\n",
    "\n",
    "# create training set\n",
    "X = all_x[indices[:100], :]\n",
    "Y = all_y[indices[:100]]\n",
    "\n",
    "# train model\n",
    "logreg = LogisticRegression(C=1e5, solver='lbfgs', multi_class='multinomial')\n",
    "logreg.fit(X, Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a trained model, we can use the model's output to create a CategoricalEvaluator object. We start by importing the CategoricalEvaluator class and instantiating an object, <code>cat_ce</code>, with the model's output. Predeval uses this data to form expectations about how future data will look."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from predeval import CategoricalEvaluator\n",
    "\n",
    "# give model output from training data to CategoricalEvaluator object\n",
    "model_output = logreg.predict(X)\n",
    "cat_ce = CategoricalEvaluator(model_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a real setting, we would give the model completely new data and pass its new predictions into `cat_ce`. Instead, we will pass the remaining data to the model. We will then ask predeval to compare these new outputs to the old outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Passed exist check; observed=[0 1 2] (Expected [0, 1, 2])\n",
      "Passed chi2 check; test statistic=0.6065, p=0.7384\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('exist', True), ('chi2', True)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# give validation data to CateogoricalEvaluator object\n",
    "new_model_output = logreg.predict(all_x[indices[100:], :])\n",
    "cat_ce.check_data(new_model_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not surprisingly, predeval does not detect a change in the model's outputs.\n",
    "\n",
    "We can also give the model's output probabilities to a ContinuousEvaluator object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from predeval import ContinuousEvaluator\n",
    "\n",
    "# give probabilities from training data to ContinuousEvaluator object\n",
    "new_model_output = logreg.predict_proba(X)\n",
    "con_ce = ContinuousEvaluator(new_model_output[:, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, we can pass the remaining samples to the ContinuousEvaluator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Passed min check; min observed=0.0121\n",
      "Passed max check; max observed=0.9315\n",
      "Passed mean check; mean observed=0.3838 (Expected 0.3500 +- 0.6873)\n",
      "Passed std check; std observed=0.2692 (Expected 0.3437 +- 0.1718)\n",
      "Passed ks check; test statistic=0.3400, p=0.0006\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('min', True), ('max', True), ('mean', True), ('std', True), ('ks', True)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# give validation data to ContinuousEvaluator object\n",
    "new_model_output = logreg.predict_proba(all_x[100:, :])\n",
    "con_ce.check_data(new_model_output[:, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, predeval does not detect any changes in the model's output. \n",
    "\n",
    "Let's say one of your features becomes corrupted and is populated entirely by 1s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed exist check; observed=[0] (Expected [0, 1, 2])\n",
      "WARNING: NOT ALL CATEGORIES PRESENT\n",
      "Failed chi2 check; test statistic=1000.0000, p=0.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('exist', False), ('chi2', False)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_x[:, 0] = 1\n",
    "new_bad_model_output = logreg.predict(all_x[indices[100:], :])\n",
    "cat_ce.check_data(new_bad_model_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, we predeval does detect a change in the model's output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed min check; min observed=0.0000\n",
      "Passed max check; max observed=0.0000\n",
      "Passed mean check; mean observed=0.0000 (Expected 0.3500 +- 0.6873)\n",
      "Failed std check; std observed=0.0000 (Expected 0.3437 +- 0.1718)\n",
      "Failed ks check; test statistic=1.0000, p=0.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('min', False), ('max', True), ('mean', True), ('std', False), ('ks', False)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_model_output = logreg.predict_proba(all_x[100:, :])\n",
    "con_ce.check_data(new_model_output[:, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, predeval detects a change in the model's outputs."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
